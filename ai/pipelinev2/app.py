import os
import cv2
import torch
import numpy as np
from PIL import Image
from torchvision.transforms import functional as F
import torchvision
import supervision as sv
from ultralytics import YOLO
import openai
from io import BytesIO
import base64

# ------------------------------
# âœ… ì„¤ì •
image_path = "C:/S12P21D101/ai/pipelinev2/photo.jpg"
yolo_model_path = "model/yolo_laptop.pt"
faster_model_path = "model/faster_damage.pth"

yolo_threshold = 0.7
faster_threshold = 0.7

image_save_path = "filtered_result_vis.jpg"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
class_names = ["background", "damage_bbox"]

client = openai.OpenAI(api_key="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")
# ------------------------------

# âœ… 1, Faster R-CNN ëª¨ë¸ ë¡œë“œ
def load_faster_model():
    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None, num_classes=2)
    model.load_state_dict(torch.load(faster_model_path, map_location=device))
    model.to(device)
    model.eval()
    print("âœ… Faster R-CNN ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    return model

# âœ… 2. YOLO ëª¨ë¸ ë¡œë“œ
def load_yolo_model():
    model = YOLO(yolo_model_path)
    print("âœ… YOLO ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    return model

# âœ… 3. Faster R-CNN ì „ì²´ ì´ë¯¸ì§€ Slicer
def run_supervision_slicer(faster_model, image):
    def callback(image_slice: np.ndarray) -> sv.Detections:
        image_pil = Image.fromarray(cv2.cvtColor(image_slice, cv2.COLOR_BGR2RGB))
        image_tensor = F.to_tensor(image_pil).unsqueeze(0).to(device)

        with torch.no_grad():
            output = faster_model(image_tensor)[0]

        boxes = output['boxes']
        scores = output['scores']
        labels = output['labels']

        keep = scores > faster_threshold

        boxes = boxes[keep].cpu().numpy()
        scores = scores[keep].cpu().numpy()
        labels = labels[keep].cpu().numpy()

        return sv.Detections(
            xyxy=boxes,
            confidence=scores,
            class_id=labels
        )

    slicer = sv.InferenceSlicer(callback=callback)
    # slicer = sv.InferenceSlicer(callback=callback, overlap_wh=(100, 100)) # ì–˜ëŠ” ê²¹ì¹˜ëŠ” ê±°
    detections = slicer(image)

    all_detections = []
    for box, score, cls_id in zip(detections.xyxy, detections.confidence, detections.class_id):
        all_detections.append({
            "bbox": [float(v) for v in box],
            "score": float(score),
            "label": class_names[cls_id]
        })

    print(f"âœ… Faster ì „ì²´ detection ê°œìˆ˜: {len(all_detections)}")
    return all_detections

# âœ… 4. YOLOë¡œ ssafy_laptop bbox ê°€ì ¸ì˜¤ê¸°
def detect_laptop_bbox(yolo_model, image_path):
    results = yolo_model(image_path, conf=yolo_threshold)
    laptop_bboxes = []

    for box in results[0].boxes:
        cls_id = int(box.cls)
        label = yolo_model.names[cls_id]
        conf = float(box.conf)
        xyxy = box.xyxy.cpu().tolist()[0]
        if label == "ssafy_laptop":
            laptop_bboxes.append([int(x) for x in xyxy])

    print(f"âœ… YOLO ssafy_laptop bbox ê°œìˆ˜: {len(laptop_bboxes)}")
    return laptop_bboxes

# âœ… 5-1. Faster ê²°ê³¼ë¥¼ YOLO bbox ë‚´ë¶€ë§Œ í•„í„°ë§ & GPT ë¶„ì„
def filter_and_classify(faster_detections, laptop_bboxes, image):
    if not laptop_bboxes:
        print("âš  ssafy_laptop bboxê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []

    X1, Y1, X2, Y2 = laptop_bboxes[0]

    results = []

    for det in faster_detections:
        box = det['bbox']
        x1, y1, x2, y2 = map(int, box)

        if x1 >= X1 and y1 >= Y1 and x2 <= X2 and y2 <= Y2:
            crop = image[y1:y2, x1:x2]
            crop_pil = Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))

            gpt_result = classify_damage_with_gpt(crop_pil)
            print(f"ğŸ¤–GPT ë¶„ì„ ê²°ê³¼: {gpt_result}")

            det['gpt_result'] = gpt_result
            results.append(det)

    print(f"âœ¨ GPT ë¶„ì„ bbox ê°œìˆ˜: {len(results)}")
    return results

# âœ… 5-2. GPT ë¶„ì„
def classify_damage_with_gpt(crop_image):
    buffer = BytesIO()
    crop_image.save(buffer, format="JPEG")
    buffer.seek(0)

    # âœ… base64ë¡œ ì´ë¯¸ì§€ encode
    base64_image = base64.b64encode(buffer.read()).decode('utf-8')

    # âœ… ìµœì‹  GPT Vision API í˜¸ì¶œ
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "ì´ë¯¸ì§€ëŠ” ë¶„ì„í•´ì„œ ë‹¤ìŒ í´ë˜ìŠ¤ë¥¼ êµ¬ë¶„í•´ì¤˜.\n 1. ìŠ¤í¬ë˜ì¹˜ì•¼ ì•„ë‹˜ í ì§‘ì´ì•¼ì•¼?\n2. ê° ì†ìƒì— ëŒ€í•´ì„œ ì†ìƒë„ë¥¼ íŒë‹¨í•´ì¤˜.(ìƒ, ì¤‘, í•˜)"},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}"
                        }
                    }
                ]
            }
        ],
        max_tokens=300
    )

    result = response.choices[0].message.content
    return result

# # âœ… 5. Faster ê²°ê³¼ë¥¼ YOLO bbox ë‚´ë¶€ë§Œ í•„í„°ë§
# def filter_by_yolo(faster_detections, laptop_bboxes):
#     if not laptop_bboxes:
#         print("âš  ssafy_laptop bboxê°€ ì—†ìŠµë‹ˆë‹¤.")
#         return []

#     X1, Y1, X2, Y2 = laptop_bboxes[0]  # laptopì´ 1ê°œë¼ê³  ê°€ì •

#     def is_inside(box):
#         x1, y1, x2, y2 = box
#         return (x1 >= X1) and (y1 >= Y1) and (x2 <= X2) and (y2 <= Y2)

#     filtered = [det for det in faster_detections if is_inside(det['bbox'])]
#     print(f"âœ¨ í•„í„°ë§ëœ bbox ê°œìˆ˜: {len(filtered)}")
#     return filtered

# âœ… 6. bbox ì´ë¯¸ì§€ ì €ì¥
def save_annotated_image(image, detections, save_path):
    box_annotator = sv.BoxAnnotator()
    label_annotator = sv.LabelAnnotator()

    sv_detections = sv.Detections(
        xyxy=np.array([det["bbox"] for det in detections]),
        confidence=np.array([det["score"] for det in detections]),
        class_id=np.zeros(len(detections), dtype=int)
    )

    labels = [det["label"] for det in detections]

    annotated_image = box_annotator.annotate(scene=image.copy(), detections=sv_detections)
    annotated_image = label_annotator.annotate(scene=annotated_image, detections=sv_detections, labels=labels)

    cv2.imwrite(save_path, annotated_image)
    print(f"âœ… bbox ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {save_path}")

# ------------------------------
# â–¶ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
if __name__ == "__main__":
    faster_model = load_faster_model()
    yolo_model = load_yolo_model()

    image = cv2.imread(image_path)

    # 1. supervisionìœ¼ë¡œ ì „ì²´ ì´ë¯¸ì§€ì—ì„œ Faster R-CNN detection
    faster_detections = run_supervision_slicer(faster_model, image)

    # 2. YOLOë¡œ ssafy_laptop bbox íƒì§€
    laptop_bboxes = detect_laptop_bbox(yolo_model, image_path)

    # 3. Faster detection ì¤‘ laptop bbox ë‚´ë¶€ ê²°ê³¼ë§Œ í•„í„°ë§
    # filtered_detections = filter_by_yolo(faster_detections, laptop_bboxes)
    filtered_detections = filter_and_classify(faster_detections, laptop_bboxes, image)

    # 4. ì´ë¯¸ì§€ ì‹œê°í™”ë§Œ ì €ì¥
    save_annotated_image(image, filtered_detections, image_save_path)
# ------------------------------